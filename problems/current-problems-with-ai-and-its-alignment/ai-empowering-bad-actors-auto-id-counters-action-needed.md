# AI empowering bad actors

#### DeepFake Attacks

Artificial intelligence is rapidly advancing, bringing both immense potential and alarming risks. As AI systems grow more sophisticated, they empower malicious actors like scammers, fraudsters, and those seeking to undermine democratic processes. Deepfake technology powered by AI can generate highly realistic yet completely fabricated video and audio content to deceive people. [In one chilling case](https://incode.com/blog/25-million-deepfake-fraud-hong-kong/), a Hong Kong finance firm fell victim to a $25 million deepfake fraud scam.

#### Problems with Biometric ID

Other networks have emerged approaching the identity problem with [personally invasive techniques](https://www.infosecurity-magazine.com/news/goldpickaxe-trojan-biometric/). WorldCoin's iris-scanning biometric identity network aim to provide a secure form of identification, yet they also raise significant privacy and civil liberties concerns. By requiring the collection of highly sensitive biometric data linked to people's physical identities, such centralized databases create risks if the information is abused by bad actors.

[Deepfake AI](https://www.gao.gov/assets/gao-20-379sp.pdf) could potentially exploit the biometric data to generate utterly convincing impersonations of specific individuals for nefarious purposes like fraud, framing innocents for crimes, or producing incendiary disinformation content designed to sow social discord. Authoritarian regimes could pair deepfakes with the biometric data to fabricate fake "evidence" persecuting dissidents or minority groups. Given AI's potent capabilities in this domain, amassing centralized biometric databases provides a concerning attack vector that malicious actors could weaponize for large-scale abuse and subjugation if accessed

#### LLM Misinformation Consequences

Large language models can also proliferate misinformation and disinformation at an unprecedented scale, with potentially catastrophic consequences during election cycles. As a U.S. Government report warned, AI-generated misinformation could severely disrupt fair elections by flooding the information landscape with fake content indistinguishable from the truth. This threat has already begun manifesting, with major tech companies like Google and OpenAI [detecting widespread AI-enabled disinformation campaigns](https://fortune.com/2024/02/27/election-misinformation-genai-chatbots-google-openai/) attempting to sway public opinion on political issues.

The ramifications of AI being weaponized by bad actors are severe and far-reaching. Safeguards must be implemented to prevent such abuses and uphold trust as AI capabilities grow exponential.
